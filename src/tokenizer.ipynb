{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit ('env')",
   "display_name": "Python 3.8.2 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "a76bd86ad8733a38d2244257d158325e379ef26034cde8e023625a5093fa4195"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this notebook we want to train a tokenizer on the sequences. A tokenizer is a technique used in the Natural Language Processing field to generate tokens from words, sentences or documents.\n",
    "\n",
    "We will use it to generate subsequences which we will use as features in modelling.\n",
    "\n",
    "Think of it as **N-GRAM technique but more far more effective that permutations of it**.\n",
    "\n",
    "You can see that this technique is **pretty fast and afordable**, **even that I am doing the whole process on my laptop**, so **anyone can do it with out the need of heavy computational power**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DEPENDENCIES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer, SentencePieceBPETokenizer\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "source": [
    "## DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train:  (63017, 41)\nTest:  (18816, 41)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/raw/train_values.csv')\n",
    "test = pd.read_csv('../data/raw/test_values.csv')\n",
    "print('Train: ',train.shape)\n",
    "print('Test: ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=63017.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49816bd58d954f5c8968e87d176a96aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18816.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64070103007d44818cd23c04a7834bb4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Put all sequences in a txt file\n",
    "filename = '../data/tokenizer/corpus.txt'\n",
    "\n",
    "with open(filename,'w+') as f:\n",
    "    for i in tqdm(range(train.shape[0]),total=train.shape[0],leave=False):\n",
    "        for x in train[\"sequence\"].values[i]:\n",
    "            f.write(x)\n",
    "        f.write('\\n')\n",
    "    for i in tqdm(range(test.shape[0]),total=test.shape[0],leave=False):\n",
    "        for x in test[\"sequence\"].values[i]:\n",
    "            f.write(x)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "source": [
    "## CREATE TOKENIZER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 21min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Time will depend on where you run it. For me, I am in a laptop.\n",
    "\n",
    "# Train tokenizer to generate vocabulary file, the vocabulary will be subsequences\n",
    "# Vocabulary size is a good hyperparameter to tune depending on your problem and sequences, just means the number of subsequences to be generated\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# You can choose which best suits you\n",
    "#tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer = SentencePieceBPETokenizer() # A bit slower\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files='../data/tokenizer/corpus.txt', vocab_size=2500, min_frequency=2,special_tokens=['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['../data/tokenizer/SP_2500/vocab.json',\n",
       " '../data/tokenizer/SP_2500/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Save tokenizer, we will use vocab file genereted for modeling\n",
    "tokenizer.save_model('../data/tokenizer/SP_2500/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use vocab file"
   ]
  }
 ]
}